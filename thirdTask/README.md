## Сингулярное разложение для сжатия BMP изображений формата 24-bit

Утилита поддерживает три реализации SVD:
- `--standard` <br>
    svd производится при помощи библиотеки NumPy, которая исходя из параметров исходной матрицы выбирает наиболее подходящий алгоритм для компрессии *(включая Метод Якоби, Двухсторонний алгоритм Ланцоша и др)*.
- `--primitive` <br>
    использует [Степенной метод](https://en.wikipedia.org/wiki/Power_iteration) для сингулярного разложения, который при всей простоте реализации обладает рядом слабых сторон, т.к. медленная сходимость для матриц с близкими собственными значениями *(на самом деле на любых матрицах он показывает себя сильно хуже остальных методов)* и чувствительность к начальному вектору.

- `--advanced` <br>
    Изначально планировалось реализовать Односторонний Метод Ланцоша и Метод Якоби, но оба этих метода давали слишком плачевные результаты *(автор до сих пор не знает в чем причина, возможно в плохой реализации)*. В итоге выбор пал на алгоритм Голуба-Рейнша, одним из главных недостатков которого является кубическая вычислительная сложность, что особенно сказывается при работе с большими матрицами.


## Эксперимент

_Стоит заметить, что для всех экспериментов коэффициент сжатия составлял K=6. Обусловлено это тем, что при меньших значениях K разница в качестве "расжатых" изображений была фактически незаметна *(хотя и для нашего K было достаточно сложно подобрать изображения с характерными изменениями)*, а сравнивать большие K нет смысла, посколку качество во всех случаях становится непростительно ужасным._

На подавляющем большинстве изображений, разницу в качестве невооруженным взгядом было просто невозможно обнаружить. Более того, сингулярные значения для всех трех реализаций совпадали в пределах погрешности.

Давайте рассмотрим примеры, на которых все же видна разница в работе методов:

### Объекты на однородном фоне (разреженность)
SIZE = 800x559
Можно заметить, что некоторые логотипы на advanced decompressed изображении имеют более яркие и характерные оттенки (особенно BBC), но качество все так же плохое.

| source | NumPy | primitive | advanced |
| -------|-------|-----------|----------|
| ![](src\logo.bmp) | ![](src\logo_standard.bmp) | ![](src\logo_primitive.bmp) | ![](src\logo_advanced.bmp) | 

### Высокая контрастность (числовая устойчивость)
SIZE = 1062x1400
Тут можно наблюдать некоторую просадку в качестве изображения после сжатия степенным методом (скорее относится к переднему плану изображения)

| source | NumPy | primitive | advanced |
| -------|-------|-----------|----------|
| ![](src\or.bmp) | ![](src\or_st.bmp) | ![](src\or_pr.bmp) | ![](src\or_ad.bmp) |

### Остальные 99.9999% случаев
В основном же качество полученных изображений буквально ничем не отличается

| source | NumPy | primitive | advanced |
| -------|-------|-----------|----------|
| ![](src\ny.bmp) | ![](src\ny_decomp_standard.bmp) | ![](src\ny_decomp_primitive.bmp) | ![](src\ny_decomp_advanced.bmp) |

### Итог
Улучшенный алгоритм не дал какого то существенного улучшения по сравнению с обычным, а оба они сильно проигрывают NumPy по времени работы, что проявляется при работе с большими изображениями. Отсюда также ясно, что если ограничить их временем работы стандартного алгоритма из NumPy, то они програют в качестве.